#DynamoDBToRedshiftConvertDataUsingHive Sample 

This sample demonstrates how you can use Data Pipeline's HiveActivity and RedshiftCopyActivity to copy data from a DynamoDB table to a Redshift table while performing data conversion using Hive (for data transformation) and S3 (for staging).  This sample was motivated by a use case where one wishes to convert the data type of one column to another data type.  In this sample, we will be converting a column from binary to base64 string.  To make this sample to work, you must ensure you have the following:

* Connection string for the destination Redshift cluster, e.g. jdbc:redshift://_hostname_:5439/_database_.
* Redshift database name.
* Redshift username and password.  This user must have write access to the table where data will be copied to.
* Redshift table name.
* DynamoDB table name.  Note that both the table name and column names must match on both sides of the copy.
* S3 location to direct log messages generated by Data Pipeline.
* Redshift table definition.

We will use the [Handling Binary Type Attributes Using the AWS SDK for Java Document API](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/JavaDocumentAPIBinaryTypeExample.html) example to generate data in a DynamoDB table.

The column mappings used in this sample are meant to match the table definition used in the above example.

##Hive queries
The following queries will be used to convert the ExtendedMessage column from binary to base64 string.
```sql
# tempHiveTable will receive the data from DynamoDB as-is
DROP TABLE IF EXISTS tempHiveTable;
# s3TempTable will contain the converted data
DROP TABLE IF EXISTS s3TempTable;
# Use the DynamoDB handler to export data.  Note this table still stores ExtendedMessage as binary.
CREATE EXTERNAL TABLE tempHiveTable (Id string,ReplyDateTime string,Message string,ExtendedMessage binary,PostedBy string)
  STORED BY 'org.apache.hadoop.hive.dynamodb.DynamoDBStorageHandler'
  TBLPROPERTIES (
    "dynamodb.table.name" = "#{myDDBTableName}", 
    "dynamodb.column.mapping" = "#{myDDBTableColMapping}"
);
# Define an external table that will contain the result of conversion.  Here "ExtendedMessage" is of type string. 
CREATE EXTERNAL TABLE s3TempTable (Id string,ReplyDateTime string,Message string,ExtendedMessage string,PostedBy string)
  ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'
  LOCATION '#{myOutputS3Loc}/#{format(@scheduledStartTime, 'YYYY-MM-dd-HH-mm-ss')}';
# Use the built-in function "base64" to write the binary data as base64 string
INSERT OVERWRITE TABLE s3TempTable SELECT Id,ReplyDateTime,Message,base64(ExtendedMessage),PostedBy FROM tempHiveTable;
```

You will need to provide the above information in the "put-pipeline-definition" command below.

##Before running the sample
To simplify the example, the pipeline uses the following EMR cluster configuration:
* Release label: emr-4.4.0
* Master instance type: m3.xlarge
* Core instance type: m3.xlarge
* Core instance count: 3

Please feel free to modify this configuration to suite your needs.

##Running this sample

```sh
$> aws datapipeline create-pipeline --name data_conversion_using_hive --unique-id data_conversion_using_hive 

# You receive a pipeline activity like this. 
#   -----------------------------------------
#   |             CreatePipeline             |
#   +-------------+--------------------------+
#   |  pipelineId |  df-0554887H4KXKTY59MRJ  |
#   +-------------+--------------------------+

#now upload the pipeline definition 

$> aws datapipeline put-pipeline-definition --pipeline-id df-0554887H4KXKTY59MRJ \
  --pipeline-definition file://samples/DynamoDBtoRedshiftHiveCSV.json \
  --parameter-values \
        myDDBTableColMapping="Id:Id,ReplyDateTime:ReplyDateTime,Message:Message,ExtendedMessage:ExtendedMessage,PostedBy:PostedBy" \
	myS3SourceColMapping="Id string,ReplyDateTime string,Message string,ExtendedMessage binary,PostedBy string" \
	myS3TargetColMapping="Id string,ReplyDateTime string,Message string,ExtendedMessage string,PostedBy string" \
	myHiveSelectColumns="Id,ReplyDateTime,Message,base64\(ExtendedMessage\),PostedBy" \
	myOutputS3Loc=<s3staging> \
	myLogUri=<s3log> \
	myDDBTableName=<DynamoDBTable> \
        myRedshiftEndpoint=<RedshiftEndpoint> \
        myRedshiftPrimaryKeys="Id,ReplyDateTime" \
        myRedshiftTable=<RedshiftTable> \
        myRedshiftCreateTableQuery="create table IF NOT EXISTS reply \(Id varchar\(2048\) not null distkey, ReplyDateTime varchar\(2048\) not null sortkey, Message varchar\(2048\), ExtendedMessage varchar\(2048\), PostedBy varchar\(2048\)\);" \
        myUsername=<username> \
        myPassword=<password>
# You receive a validation messages like this

#   ----------------------- 
#   |PutPipelineDefinition|
#   +-----------+---------+
#   |  errored  |  False  |
#   +-----------+---------+

#now activate the pipeline
$> aws datapipeline activate-pipeline --pipeline-id df-0554887H4KXKTY59MRJ


#check the status of your pipeline 

$> aws datapipeline list-runs --pipeline-id df-0554887H4KXKTY59MRJ
#       Name                                                Scheduled Start      Status                 
#       ID                                                  Started              Ended              
#---------------------------------------------------------------------------------------------------
#   1.  DynamoDBInputDataNode                               2016-03-31T23:38:34  FINISHED               
#       @DynamoDBInputDataNode_2016-03-31T23:38:34          2016-03-31T23:38:38  2016-03-31T23:38:39
# 
#   2.  EmrCluster1                                         2016-03-31T23:38:34  CREATING               
#       @EmrCluster1_2016-03-31T23:38:34                    2016-03-31T23:38:39                     
# 
#   3.  RedshiftCluster1                                    2016-03-31T23:38:34  WAITING_ON_DEPENDENCIES
#       @RedshiftCluster1_2016-03-31T23:38:34               2016-03-31T23:38:38                     
# 
#   4.  S3StagingDataNode                                   2016-03-31T23:38:34  WAITING_ON_DEPENDENCIES
#       @S3StagingDataNode_2016-03-31T23:38:34              2016-03-31T23:38:38                     
# 
#   5.  TableBackupActivity                                 2016-03-31T23:38:34  WAITING_FOR_RUNNER     
#       @TableBackupActivity_2016-03-31T23:38:34            2016-03-31T23:38:38                     
```

##Related documentation
* [HiveActivity](http://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-hiveactivity.html)
* [RedshiftCopyActivity](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-object-redshiftcopyactivity.html)

